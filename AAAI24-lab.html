<!DOCTYPE html>
<!--[if IE 8 ]>
<html class="no-js oldie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]>
<html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html> <!--<![endif]-->
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body, html {
  height: 100%;
  margin: 0;
  font-family: Arial, Helvetica, sans-serif;
}

.hero-image {
  background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url("images/banner2.png");
  height: 50%;
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
  position: relative;
}

.hero-text {
  text-align: center;
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  color: white;
}

.hero-text button {
  border: none;
  outline: 0;
  display: inline-block;
  padding: 10px 25px;
  color: black;
  background-color: #ddd;
  text-align: center;
  cursor: pointer;
}

.hero-text button:hover {
  background-color: #555;
  color: white;
}

.section-head-text {
  color: rgba(56, 66, 78, 0.5);
  text-align: center; 
  font-size: 1.6rem;
  line-height: 1.875;
  margin-bottom: 0.3rem;
  letter-spacing: .1rem;
}

.section-text {
  max-width: 80%;
  height: 1.5em;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}
</style>

  
</head>

<body>

<div class="hero-image">
  <div class="hero-text">
    <h1 style="font-size:50px">LH2: Introduction to MDP Modeling and Interaction via RDDL and pyRDDLGym</h1>
    <p>AAAI-2024 Vancouver</p>
  </div>
</div>

<section>
  <div class="section-head-text">
    <h2>Overview</h2>
  </div>
  <p>
    RDDL (pronounced as “riddle”) stands for the Relational Dynamic Influence Diagram Language. 
    It is the domain modeling language utilized in the International Conference on Automated Planning and Scheduling (ICAPS) in the years 2011, 2014, 2018, 
    and most recently in 2023 for the Probabilistic Planning and Reinforcement Learning track of the International Planning Competitions. 
    RDDL was designed to efficiently represent real-world stochastic planning problems, specifically Markov Decision Processes (MDPs), with a focus on factored MDPs characterized by highly structured transition and reward functions. 
    This tutorial aims to provide basic understanding of RDDL, including recent language enhancements and features, through a practical example. 
    We will introduce a problem based on a real-world scenario and incrementally build up its representation in RDDL, starting from the raw mathematical equations up to a full RDDL description. 
    Furthermore, we will introduce “pyRDDLGym,” a new Python framework for the generation of Gym environments from RDDL descriptions. 
    This facilitates interaction with Reinforcement Learning (RL) agents via the standard Gym interface, as well as enables planning agents to work with the model. 
    In a series of exercises, we will explore the capabilities of pyRDDLGym, which includes generation of the Dynamic Bayesian Networks (DBN) and eXtended Algebraic Decision Diagrams (XADD)-based conditional probability functions, 
    as well as offering both generic and custom visualization options. We will also generate a functional environment for the example problem. 
    To close the loop from a mathematical representation to a fully operational policy, we will utilize the built-in model-based anytime backpropagation planner, 
    known as “JaxPlan.” This will enable us to obtain solutions for the example problem, effectively closing the gap between theoretical description and a practical working policy.
  </p>
</section>

<section>
  <h2 style="color: rgba(56, 66, 78, 0.5);text-align: center; font-size: 1.6rem;line-height: 1.875;margin-bottom: 0.3rem;letter-spacing: .2rem;">Schedule</h2>
  <p></p>
</section>

<section>
  <h2 style="color: rgba(56, 66, 78, 0.5);text-align: center; font-size: 1.6rem;line-height: 1.875;margin-bottom: 0.3rem;letter-spacing: .2rem;">Organizers</h2>
  <p></p>
</section>
  

</body>
</html>


  
